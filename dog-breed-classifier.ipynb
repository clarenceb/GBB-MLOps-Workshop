{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Jupyter Notebook Guide - Dog Breed Classification Using PyTorch, Azure ML, and Visual Studio Code\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Have you ever seen a dog and not been able to tell the breed? Some dogs look so similar, that it can be nearly impossible to tell. For instance these are a few breeds that are difficult to tell apart:\n",
    "\n",
    "#### Alaskan Malamutes vs Siberian Huskies\n",
    "![Image of Alaskan Malamute vs Siberian Husky](http://cdn.akc.org/content/article-body-image/malamutehusky.jpg)\n",
    "\n",
    "#### Whippet vs Italian Greyhound \n",
    "![Image of Whippet vs Italian Greyhound](http://cdn.akc.org/content/article-body-image/whippetitalian.jpg)\n",
    "\n",
    "There are sites like https://www.bing.com/visualsearch/Microsoft/WhatDog, which use Microsoft Cognitive Services to be able to make this easier. \n",
    "\n",
    "In this tutorial, you will learn how to train your own image classification model using [transfer learning](https://cs231n.github.io/transfer-learning/). The Azure Machine Learning Python SDK's [PyTorch estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-pytorch) enables you to easily submit PyTorch training jobs for both single-node and distributed runs on Azure compute. The model is trained to classify dog breeds using a pretrained ResNet18 model that has been trained on the [Stanford Dog dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/). This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization.\n",
    "\n",
    "In the interest of time, we will use a subset of this dataset which includes 10 dog breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning service?\n",
    "\n",
    "Azure Machine Learning Service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](https://docs.microsoft.com/en-us/azure/machine-learning/service/media/overview-what-is-azure-ml/aml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Sign up for an [Azure account](https://azure.microsoft.com/en-us/free/).\n",
    "    * In this workshop, you can choose to use your own account or use the Azure AD credentials provided to you.\n",
    "2. Verify that the [Azure Machine Learning SDK](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py) is installed by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check Azure ML core SDK version number\n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train image classification models\n",
    "Training machine learning models, particularly deep neural networks, is often a time- and compute-intensive task. Once you've finished writing your training script and running on a small subset of data on your local machine, you will likely want to scale up your workload. The Azure Machine Learning (AML) service enables users to easily train their models in the Azure ecosystem, and it also provides built-in support for popular machine learning frameworks, such as PyTorch and TensorFlow, to simplify the workflow.\n",
    "\n",
    "### Steps to train models:\n",
    "\n",
    "1. Connect to an Azure Machine Learning service Workspace \n",
    "2. Create a remote compute cluster for training your models\n",
    "3. Upload your training data to a remote data store\n",
    "4. Prepare your Python training script\n",
    "5. Submit a training job to AML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to an Azure Machine Learning service Workspace\n",
    "You will need to create or connect to an AML [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) in order to train your models.  Replace any placeholders in the Python code cells below with your relevant details.\n",
    "\n",
    "**You will be asked to login during this step. Please use the Azure AD credentials provided to you.**\n",
    "\n",
    "* If you have trouble authenticating, see [this guide](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azure-ml.ipynb) for alternative authencation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if you are creating a new workspace\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "subscription_id = \"<your-subscription-id>\"\n",
    "resource_group = \"DogBreeds\"\n",
    "workspace_name = \"DogBreeds\"\n",
    "workspace_region = \"<your-region>\"  # e.g. useast or australiaeast\n",
    "\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group, \n",
    "                      location = workspace_region,\n",
    "                      create_resource_group = True,\n",
    "                      exist_ok = True)\n",
    "ws.write_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Otherwise, run this code to connect to an existing workspace\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a remote compute cluster for training your models\n",
    "For this tutorial, you will prepare an AML Compute cluster with NC-series GPU virtual machines to use as the [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) to execute your training script on. \n",
    "\n",
    "The following code creates a cluster for you if it does not already exist in your workspace. Otherwise, it will skip the cluster creation process.\n",
    "\n",
    "**Creation of the cluster may take approximately 5 minutes (if min_nodes > 0).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"TrainingCluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',   # Standard_NC6s_v2\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=2,\n",
    "                                                           idle_seconds_before_scaledown='300')\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "    # Use the 'status' property to get a detailed status for the current cluster. \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload your training data to a remote data store\n",
    "\n",
    "In this tutorial, we will use 10 classes of images for training. Each class represents one dog breed and consists ~150 images: 100 are training images for dog breeds, and ~50 are validation images for each class. You can view the data used [here](breeds-10). \n",
    "\n",
    "First, download the dataset (located [here](breeds-10.zip) as a zip file) locally to your current directory and extract the files. This will create a folder called breeds-10 with two subfolders train and val that contain the training and validation images, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from zipfile import ZipFile\n",
    "\n",
    "download_url = 'https://github.com/clarenceb/GBB-MLOps-Workshop/blob/master/breeds-10.zip'\n",
    "data_file = './breeds-10.zip'\n",
    "\n",
    "if os.path.isfile(data_file):\n",
    "    print(f'Skipping download, {data_file} already exists locally.')\n",
    "else:\n",
    "    urllib.request.urlretrieve(download_url, filename=data_file)\n",
    "\n",
    "    # extract files\n",
    "    with ZipFile(data_file, 'r') as zip:\n",
    "        print('extracting files...')\n",
    "        zip.extractall()\n",
    "        print('done')\n",
    "    \n",
    "    # delete zip file\n",
    "    #os.remove(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data accessible for remote training, you will need to upload the data from your local machine to the cloud. AML provides a convenient way to do so via a [Datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data). The datastore provides a mechanism for you to upload/download data, and interact with it from your remote compute targets.\n",
    "\n",
    "Each workspace is associated with a [default datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data#use-the-default-datastore-in-your-workspace). In this tutorial, we will upload the training data to this default datastore. The following code will upload the training data to the path `./breeds-10` on the default datastore.\n",
    "\n",
    "**Note: If your data is already stored in Azure, or you download the data as part of your training script, you will not need to do this step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./breeds-10', target_path='breeds-10')\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a reference to the path on the datastore with the training data. We can do so using the `path` method. In the next section, we can then pass this reference to our training script's `--data_dir` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_on_datastore = 'breeds-10'\n",
    "ds_data = ds.path(path_on_datastore)\n",
    "print(ds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you already have a datastore uploaded you can access it through the following code:**\n",
    "\n",
    "```python\n",
    "from azureml.core.datastore import Datastore\n",
    "ds = Datastore.get(ws,\"breeds-10\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Data (Optional)**\n",
    "\n",
    "If you are interested in downloading the data locally, you can run `ds.download(\"./\", 'breeds-10')`. This might take several minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare your training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a project directory that will contain all the necessary code from your local machine that you will need access to on the remote compute resource. This includes the training script and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-dog-breeds-10'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "In this tutorial, the training script is already provided for you at `pytorch_train_10.py`. In practice, you should be able to take any custom training script as is and run it with AML without having to modify your code.\n",
    "\n",
    "However, if you would like to use AML's [tracking and metrics](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-track-experiments) capabilities, you will have to add a small amount of AML code inside your training script. \n",
    "\n",
    "In `pytorch_train_10.py`, we will log some metrics to our AML run. To do so, we will access the AML `Run` object within the script:\n",
    "\n",
    "```python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "Further within `pytorch_train_10.py`, we log the learning rate and momentum parameters, and the best validation accuracy the model achieves:\n",
    "\n",
    "```python\n",
    "run.log('lr', np.float(learning_rate))\n",
    "run.log('momentum', np.float(momentum))\n",
    "\n",
    "run.log('best_val_acc', np.float(best_acc))\n",
    "```\n",
    "\n",
    "**Note** These metrics can be view in the [Azure portal](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-track-experiments#view-the-experiment-in-the-azure-portal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, copy the training script `pytorch_train_10.py` into your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('pytorch_train_10.py', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submit a training job to AML\n",
    "Now that you have your data and training script prepared, you are ready to train on your remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "An experiment is a grouping of many runs from a specified script. Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this transfer learning PyTorch tutorial. \n",
    "\n",
    "**Please enter your own unique name so that you can track your specific runs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "#experiment_name = <<ENTER UNIQUE NAME HERE>>\n",
    "experiment_name = 'pytorch-dogs'\n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch estimator\n",
    "The AML SDK's PyTorch [estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-ml-models) enables you to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch). The following code will define a [single-node](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-ml-models#single-node-training) PyTorch job.  Refer to the docs for how to run [distributed training on a multi-node cluster](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-ml-models#distributed-training-and-custom-docker-images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "script_params = {\n",
    "    '--data_dir': ds_data.as_mount(),\n",
    "    '--num_epochs': 10,\n",
    "    '--output_dir': './outputs'\n",
    "}\n",
    "\n",
    "estimator = PyTorch(source_directory=project_folder, \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target, \n",
    "                    entry_script='pytorch_train_10.py',\n",
    "                    use_gpu=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `script_params` parameter is a dictionary containing the command-line arguments to your training script `entry_script`. Please note the following:\n",
    "- We passed our training data reference `ds_data` to our script's `--data_dir` argument. This will 1) mount our datastore on the remote compute and 2) provide the path to the training data `breeds-10` on our datastore.\n",
    "- We specified the output directory as `./outputs`. The `outputs` directory is specially treated by AML in that all the content in this directory gets uploaded to your workspace as part of your run history. The files written to this directory are therefore accessible even once your remote run is over. In this tutorial, we will save our trained model to this output directory.\n",
    "\n",
    "To leverage the Azure VM's GPU for training, we set `use_gpu=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the training job\n",
    "Now let's submit the experiment run to the AML service. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)\n",
    "print(run.get_details())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "Once the run is submitted, you can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The run will take a few minutes - check the widget above for its status.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model as a web service to ACI\n",
    "\n",
    "Once the wiget shows the run is compete, we are ready to deploy this model as a web service for predictions.\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "1. Register the model with AML\n",
    "2. Deploy the model as a web service to [Azure Container Instances](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#aci) (suitable for DEV-TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Register the model with AML\n",
    "Model registration allows you to store and version your models in the Azure cloud, in your workspace. The model registry makes it easy to organize and keep track of your trained models.\n",
    "\n",
    "**Please use a unique name for the model**. You will need to edit the `init()` function in the pytorch_score.py file to match the unique name used. Change the line `model_path = Model.get_model_path('dogs')` to use the unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = run.register_model(model_name = 'dogbreedmodel', model_path = 'outputs/model.pt')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## if you need to re-reference the run object specifically uncomment this section\n",
    "\n",
    "# from azureml.core import Run\n",
    "# run = Run(experiment, run_id=\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deploy model as web service to Azure Container Instances (ACI)\n",
    "\n",
    "Once you have your trained model, you can deploy the model on Azure. You can deploy your trained model as a web service on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), IoT edge device, or Field Programmable Gate Arrays (FPGAs). ACI is generally cheaper than AKS and can be set up in 4-6 lines of code. ACI is the perfect option for testing deployments. Later, when you're ready to use your models and web services for high-scale, production usage, you can deploy them to AKS.\n",
    "\n",
    "**Note** ACI is intended for DEV-TEST and AKS for production.  AKS has a requirement of 12 cores (virtual CPUs) per cluster.  You can lift this requirement in DEV-TEST like so:\n",
    "\n",
    "```python\n",
    "# Attach the cluster to your workgroup. If the cluster has less than 12 virtual CPUs, use the following instead:\n",
    "# attach_config = AksCompute.attach_configuration(resource_group = resource_group,\n",
    "#                                         cluster_name = cluster_name,\n",
    "#                                         cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)\n",
    "```\n",
    "\n",
    "In this tutorial, we will deploy the model as a web service in [Azure Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/) (ACI). \n",
    "\n",
    "For more information on deploying models using Azure ML, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the scoring script\n",
    "\n",
    "First, we will create a scoring script that will be invoked by the web service call. Note that the scoring script must have two required functions - required when packaged as a Docker image for deployment:\n",
    "\n",
    "* `init()`: In this function, you typically load the model into a `global` object. This function is executed only once when the Docker container is started. \n",
    "* `run(input_data)`: In this function, the model is used to predict a value based on the input data. The input and output typically use JSON as serialization and deserialization format, but you are not limited to that.\n",
    "\n",
    "Here's the scoring file that our web service will use this file to predict the breed of the dog in the image. \n",
    "\n",
    "**IMPORTANT: update the model name to match yours.**\n",
    "\n",
    "When writing your own scoring script, don't forget to test it locally first before you go and deploy the web service.\n",
    "\n",
    "**Note: this script is already part of this repository.  Running the code cell below will overwrite the file with any changes you've made.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytorch_score.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def preprocess_image(image_file):\n",
    "    \"\"\"Preprocess the input image.\"\"\"\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    image = data_transforms(image).float()\n",
    "    image = torch.tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def base64ToImg(base64ImgString):\n",
    "    base64Img = base64ImgString.encode('utf-8')\n",
    "    decoded_img = base64.b64decode(base64Img)\n",
    "    return BytesIO(decoded_img)\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('dogbreedmodel')\n",
    "    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "def run(input_data):\n",
    "    img = base64ToImg(json.loads(input_data)['data'])\n",
    "    img = preprocess_image(img)\n",
    "\n",
    "    # get prediction\n",
    "    output = model(img)\n",
    "\n",
    "    classes = ['Chihuahua',\n",
    "            'Italian_greyhound',\n",
    "            'whippet',\n",
    "            'golden_retriever',\n",
    "            'Shetland_sheepdog',\n",
    "            'German_shepherd',\n",
    "            'boxer',\n",
    "            'Saint_Bernard',\n",
    "            'malamute',\n",
    "            'Siberian_husky']\n",
    "    ## If you try with 20 classes please uncomment this:\n",
    "#    classes =['Chihuahua',\n",
    "#             'Italian_greyhound',\n",
    "#             'whippet',\n",
    "#             'Yorkshire_terrier',\n",
    "#             'golden_retriever',\n",
    "#             'Labrador_retriever',\n",
    "#             'Shetland_sheepdog',\n",
    "#             'Border_collie',\n",
    "#             'German_shepherd',\n",
    "#             'Bernese_mountain_dog',\n",
    "#             'boxer',\n",
    "#             'bull_mastiff',\n",
    "#             'French_bulldog',\n",
    "#             'Great_Dane',\n",
    "#             'Saint_Bernard',\n",
    "#             'Siberian_husky',\n",
    "#             'basenji',\n",
    "#             'pug',\n",
    "#             'Samoyed',\n",
    "#             'Pembroke'\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    pred_probs = softmax(model(img)).detach().numpy()[0]\n",
    "    index = torch.argmax(output, 1)\n",
    "\n",
    "    result = json.dumps({\"label\": classes[index], \"probability\": str(pred_probs[index])})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment file\n",
    "Next, we will need to create an environment file (`myenv.yml`) that specifies all of the scoring script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image by AML. In this case, we need to specify `torch`, `torchvision`, `pillow`, and `azureml-sdk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile myenv.yml\n",
    "name: myenv\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - pip:\n",
    "    - torch\n",
    "    - torchvision\n",
    "    - pillow\n",
    "    - azureml-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the ACI container where the service runs in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure the container image\n",
    "from azureml.core.image import ContainerImage\n",
    "image_config = ContainerImage.image_configuration(execution_script='pytorch_score.py', \n",
    "                                                  runtime='python', \n",
    "                                                  conda_file='myenv.yml',\n",
    "                                                  description='Image with dog breed model')\n",
    "# Configure the ACI container\n",
    "# Note: This will create a public IP endpoint for your scoring URI.  If you have blocked public IPs this won't work.\n",
    "from azureml.core.webservice import AciWebservice\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1,\n",
    "                                               location='<your_preferred_region>', # e.g. useast, westus, australiaeast\n",
    "                                               tags={'data': 'dog_breeds', 'method':'transfer learning', 'framework':'pytorch'},\n",
    "                                               description='Classify dog breeds using transfer learning with PyTorch')\n",
    "# Retrieve the model from your workspace.\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model(ws, name='dogbreedmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Deploy the registered model\n",
    "Finally, let's deploy a web service from our registered model. \n",
    "Then, deploy the web service using the ACI config and image config files created in the previous steps. We pass the `model` object in a list to the `models` parameter. If you would like to deploy more than one registered model, append the additional models to this list.\n",
    "\n",
    "**Please use a unique service name.**\n",
    "\n",
    "**Note**: `Webservice.deploy_from_model` creates a new Docker image every time (which can take a long time).  If you have a previously created image that you want to deploy, use: [`WebService.deploy_from_image`](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice(class)?view=azure-ml-py#deploy-from-image-workspace--name--image--deployment-config-none--deployment-target-none-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "service_name = 'dogbreedsvc'\n",
    "service = Webservice.deploy_from_model(workspace=ws,\n",
    "                                       name=service_name,\n",
    "                                       models=[model],\n",
    "                                       image_config=image_config,\n",
    "                                       deployment_config=aciconfig,)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: If your deployment fails for any reason, the first thing to look at is the logs from the service by running the following command: `service.get_logs()`. If you need to redeploy, make sure to delete the service before you do so: `service.delete()`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's test our web service!\n",
    "Finally, let's test our deployed web service. We will send the image data as a base64-encoded JSON string to the web service hosted in ACI and use the SDK's `run` API to invoke the service. Here we will take an arbitrary image from online to predict on (or uncomment a URL below to a specific image). This is the same as above, but now we are testing on our own trained model. You can use any dog image, but please remember we only trained on 10 classes so an uknown class will yield unreliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install skimage, if needed\n",
    "!pip install --upgrade scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# importing the requests library \n",
    "import requests \n",
    "import os, json, base64\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "##Get random dog\n",
    "def get_random_dog():\n",
    "    r = requests.get(url =\"https://dog.ceo/api/breeds/image/random\")\n",
    "    URL= r.json()['message']\n",
    "    return URL\n",
    "\n",
    "##Get Random Dog Image\n",
    "URL = get_random_dog()\n",
    "\n",
    "##whippet Example \n",
    "#URL=\"https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2017/11/12223018/Whippet-On-White-03.jpg\"\n",
    "\n",
    "##italian greyhound Example\n",
    "# URL=\"https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2017/11/12231757/Italian-Greyhound-On-White-03.jpg\"\n",
    "\n",
    "##chihuahua Example\n",
    "# URL =\"https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2017/11/12213613/Chihuahua-onWhite-13.jpg\"\n",
    "\n",
    "##whippet\n",
    "# URL=\"https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2017/11/12223018/Whippet-On-White-03.jpg\"\n",
    "\n",
    "##italian greyhound\n",
    "# URL=\"https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2017/11/12231757/Italian-Greyhound-On-White-03.jpg\"\n",
    "\n",
    "with urllib.request.urlopen(URL) as url:\n",
    "    test_img = io.BytesIO(url.read())\n",
    "\n",
    "# ## If you downloaded the dataset, you can try this arbitrary image from the test dataset\n",
    "# # test_img = os.path.join('breeds-10', 'val', 'n02085620-Chihuahua', 'n02085620_1271.jpg') \n",
    "\n",
    "plt.imshow(Image.open(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "service_name = 'dogbreedsvc'\n",
    "service = Webservice(ws, service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def imgToBase64(img):\n",
    "    \"\"\"Convert pillow image to base64-encoded image\"\"\"\n",
    "    imgio = BytesIO()\n",
    "    img.save(imgio, 'JPEG')\n",
    "    img_str = base64.b64encode(imgio.getvalue())\n",
    "    return img_str.decode('utf-8')\n",
    "\n",
    "base64Img = imgToBase64(Image.open(test_img))\n",
    "\n",
    "result = service.run(input_data=json.dumps({'data': base64Img}))\n",
    "print(json.loads(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Test the web service from within Visual Studio Code\n",
    "\n",
    "Now let's write a Python script in Visual Studio Code to call the web service for predicting dog breeds. \n",
    "\n",
    "Here are the steps we will go through in VS Code:\n",
    "1. Import a Jupyter Notebook\n",
    "2. Explore the AML workspace\n",
    "3. Test the web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisites\n",
    "\n",
    "0.1. Visual Studio Code is a free, lightweight, cross-platform code editor. [Install Visual Studio Code](https://code.visualstudio.com/) if you don't have it installed.\n",
    "![VS Code](screenshots/VSCode.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.2. Install the Microsoft Python extension in VS Code.\n",
    "![Python extension](screenshots/PythonExt.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3. Install the Azure Machine Learning extension in VS Code.\n",
    "![AML extension](screenshots/AMLExt.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import a Jupyter Notebook\n",
    "\n",
    "First, [Download this Jupyter Notebook](test-in-vscode.ipynb) onto your local machine. This notebooks contains the same code we used above to test the web service, but in a separate notebook file. Now we will import this notebook file into VS Code.\n",
    "\n",
    "Launch VS Code, use menu `File->Open Folder` to open the folder where you save the Jupyter Notebook file. \n",
    "![Open Folder in VS Code](screenshots/OpenFolder.png)\n",
    "\n",
    "In the EXPLORER window in VS Code, click on the `test-in-vscode.ipynb` file, and you should see a notification message in the bottom right corner. Choose `Yes` to import the Notebook into Python code.\n",
    "![Import Jupyter Notebook into Python code in VS Code](screenshots/Import.PNG)\n",
    "\n",
    "Import takes a few seconds. Once finished, you will have a Python file created like the following:\n",
    "![Import finished](screenshots/ImportDone.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore the AML workspace\n",
    "Before we can test the web service in VS Code, we need to fill in the service uri that we will be testing. The [Azure Machine Learning extension for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.vscode-ai) provides an easy way to build, train, and deploy machine learning models to the AML service. Now let's use it to find out what the service uri is for the service we just deployed.\n",
    "\n",
    "Click on the `Azure` tab in the VS Code Activity bar. Use the AAD credentials provides to you to log in. Expand the `DogBreeds` workspace, and look for your service under `Deployments`. Then right click on your service and choose `View Service Properties`.\n",
    "![Find service](screenshots/FindService.PNG)\n",
    "\n",
    "In the Properties page, find the `scoringUri` on line 78. Copy the value to clipboard.\n",
    "![ScoringUri](screenshots/ScoringURI.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test the web service\n",
    "Now switch back to the Python file that was imported, paste the `scoringUri` value at line 39.\n",
    "![Paste ScoringUri in the Python code](screenshots/PasteScoringURI.PNG)\n",
    "\n",
    "Now we are ready to run the Python code to send a dog image to the web service to predict the dog breed. To better visualize the program results, we are going to run the code in the `Python Interactive` window, which brings the power of Jupyter Notebooks into VS Code.\n",
    "\n",
    "Scroll to the top of the Python file, and click on the `Run Cell` link. This will launch a Jupyter server on the local machine, which will then be used to execute the code. Results are sent back to VS Code for presentation.\n",
    "![Run Cell](screenshots/RunCell.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you don't have Jupyter installed on your local machine**, you will be prompted with this message. \n",
    "Please follow the steps here: [Installing Jupyter Notebook\n",
    "](https://jupyter.readthedocs.io/en/latest/install.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the cell finished running, you will see a dog image along with the code in the `Python Interactive` window: \n",
    "![First cell result](screenshots/1stCell.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to run the second cell, which sends back the prediction result:\n",
    "![Second cell result](screenshots/2ndcell.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete web service\n",
    "Once you no longer need the web service, you should delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up resources\n",
    "When you have finished with the workshop, you can clean up remaining resources to save on costs.\n",
    "Delete your Resource Group `DogBreeds` (or the name you chose) from the Azure Portal to remove all Azure Resources created for this workshop.\n",
    "\n",
    "If you plan to complete [Part 2 - MLOps Guide](./mlops.md) then **do not** delete your resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
